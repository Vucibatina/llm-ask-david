# Ask David ‚Äì Local RAG Chat Assistant for David Snyder's Teachings

This is a local Retrieval-Augmented Generation (RAG) application that allows users to ask questions and receive context-aware answers based on the teachings of **David Snyder**, using only his **freely available YouTube content**.

The system runs entirely on your local machine and uses:
- A **quantized LLaMA 2 model** for generating answers
- A **FAISS vector database** for fast semantic search
- **YouTube transcripts** to locate the source of answers
- **A lightweight frontend** for users to interact via the browser

---

## üõ†Ô∏è How It Works

1. The user types a question into a web interface.
2. The question is encoded using the `all-mpnet-base-v2` SentenceTransformer model.
3. A FAISS index is searched to retrieve the most relevant transcript chunks.
4. The selected chunks are passed to a local LLaMA 2 7B model (`gguf` format) via `llama-cpp-python`.
5. The model generates a response using the retrieved context.
6. The app then:
   - Shows the answer
   - Lists the matched YouTube links (with timestamp)
   - Optionally stores the full Q&A interaction in a local JSON file

---

## üìÅ Project Structure

```
ask_david/
‚îú‚îÄ‚îÄ llm_models/
‚îÇ   ‚îî‚îÄ‚îÄ llama-2-7b-chat-hf-q4_k_m.gguf       # Quantized LLaMA model for local inference
‚îÇ
‚îú‚îÄ‚îÄ rag_cache/
‚îÇ   ‚îú‚îÄ‚îÄ faiss.index                          # FAISS vector index of all embedded transcript chunks
‚îÇ   ‚îî‚îÄ‚îÄ metadata.pkl                         # Metadata for locating each chunk in the original text
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ DavidSnyder/                         # Raw transcript files (text format, one per video)
‚îÇ   ‚îî‚îÄ‚îÄ qanda/                               # JSON logs of all questions/answers (if space permits)
‚îÇ
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îî‚îÄ‚îÄ ask_david.html                       # Frontend UI for asking questions
‚îÇ
‚îú‚îÄ‚îÄ server.py                                # FastAPI backend for inference and routing
‚îî‚îÄ‚îÄ README.md                                # This file
```

---

## üîç Example Use Case

**Question:**
> "How can I overcome fear using David Snyder‚Äôs NLP techniques?"

**System Response:**
- A text response generated in David‚Äôs voice/style
- 2‚Äì3 YouTube links to exact moments where that material was covered
- The entire interaction saved locally as a timestamped JSON file

---

## üíæ Disk Space Behavior

To prevent the system from consuming excessive storage:

- It **only saves** the Q&A JSON files if **more than 500MB** of disk space is free.
- Each file is named using a Unix timestamp, e.g., `1713748123.json`.

---

## üì¶ Requirements

Make sure you have Python 3.8 or later. Install dependencies with:

```bash
pip install fastapi uvicorn sentence-transformers faiss-cpu llama-cpp-python youtube-transcript-api
```

### Optional:
- Install `ffmpeg` if you plan to process video/audio files later.

---

## üß† Data Flow Summary

- **User question** ‚Üí embedded via SentenceTransformer
- **Similarity search** ‚Üí FAISS index ‚Üí retrieves relevant transcript chunks
- **Prompt creation** ‚Üí [INST] format with context
- **Answer generation** ‚Üí LLaMA model via `llama-cpp-python`
- **Source alignment** ‚Üí YouTube timestamps using transcript matching
- **Display & logging** ‚Üí Results shown in browser + optional JSON log

---

## üì¶ Manual Setup for Large Files

Due to GitHub's 100MB file size limit, this repo does **not** include large binary files like the LLaMA model or FAISS index.

After cloning the repository, you must manually add these files:

- `llm_models/llama-2-7b-chat-hf-q4_k_m.gguf`
- `rag_cache/faiss.index` (or `index.faiss` depending on your setup)

You can:
- Transfer them via **SSH** or **scp**
- Use a shared folder like Dropbox, Google Drive, or external storage
- Or download from your own private hosting if you‚Äôve set that up

These files are required to run the application locally.

---

## Running a youtube video files transcriber

Big part of the data training pipeline is having transcribed video data pulled
from youtube.  To be able to do this, here is how to run the transcriber where you
will need to specifiy the target directory and youtube creator name so that all
their videos can be pulled and transcribed.  Here is the example on how to run:

`python transcript_loader.py --path "data/DavidSnyder" --username "DavidSnyderNLP"`

---

## Running a RAG Creation

Creating a RAG is necessary for the process to run.  Files are either copied into rag_cache
directory or generated.  In case you want to generate it, use the below command to do it
`python build_rag.py --data_dir "/Users/vuk/projects/ask_david_clean/data/DavidSnyder" --model_path "/Users/vuk/projects/llm_models/llama-2-7b-chat-hf-q4_k_m.gguf" --save_dir "rag_cache"`

--

## ‚úâÔ∏è Author

**Produced by [Vuk Radovic](mailto:vucibatina@hotmail.com)**  
Built with ‚ù§Ô∏è to make David Snyder‚Äôs material searchable, accessible, and useful in everyday life.
---

## üåê How to Access the Web Interface

To use the app through your browser, open this file locally:

```
static/ask_david.html
```

This HTML file provides a simple frontend for interacting with the backend API. You must have the FastAPI server (`server.py`) running locally before using the interface.
